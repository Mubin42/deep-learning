{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import Section"
      ],
      "metadata": {
        "id": "i0JR3lNfeifp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpFhl07reeQw"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import math\n",
        "\n",
        "\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# from tensorflow.keras import Model, Sequential\n",
        "\n",
        "# from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_policy(policy)\n",
        "print('Compute dtype: %s' % policy.compute_dtype)\n",
        "print('Variable dtype: %s' % policy.variable_dtype)"
      ],
      "metadata": {
        "id": "FUtnzqQwjc-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13aa9ddf-eb23-4b21-d452-60d59a1a1711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/mixed_precision/loss_scale.py:52: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compute dtype: float16\n",
            "Variable dtype: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting Google Colab"
      ],
      "metadata": {
        "id": "4pZmIOQjj6aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t3fgiy-2j-Ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8787be-9761-4929-a3ad-937b39b94617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assigning Directories"
      ],
      "metadata": {
        "id": "7S_anLR_j_AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory      = \"/content/drive/MyDrive/Extra/train.csv\""
      ],
      "metadata": {
        "id": "SM112Qnkj-iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We have two tasks.\n",
        "Task-1: Recognize each bangla character. Multi-class classification.\n",
        "\n",
        "Task-2: Recognize male or female handwritting. Binary Classification.\n",
        "\n",
        "Example Code - https://towardsdatascience.com/multi-task-learning-for-computer-vision-classification-with-keras-36c52e6243d2"
      ],
      "metadata": {
        "id": "WV4hw0SEhcb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (28,28)\n",
        "batch_size = 8"
      ],
      "metadata": {
        "id": "bbX6Dv2ThtC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Description\n",
        "\n",
        "All Images are 28x28 pixels.\n",
        "\n",
        "Dataset devided into train, test, validation"
      ],
      "metadata": {
        "id": "wEhPDAa1h5dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Separating Data and Label\n",
        "Y_train = train[\"label\"]\n",
        "X_train = train.drop(labels = [\"label\"],axis = 1)\n",
        "\n",
        "# Normalize the data\n",
        "X_train = X_train / 255.0\n",
        "\n",
        "# Reshape the array into 28 x 28 pixel\n",
        "X_train = X_train.values.reshape(-1,28,28,1)"
      ],
      "metadata": {
        "id": "R2vJElsFl-Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # experimental\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "train_generator = tf.keras.utils.image_dataset_from_directory(train_directory,\n",
        "                                                              labels='inferred',\n",
        "                                                              color_mode='grayscale',\n",
        "                                                            shuffle=True,\n",
        "                                                            batch_size=batch_size,\n",
        "                                                            image_size=image_size)\n",
        "\n",
        "test_generator = tf.keras.utils.image_dataset_from_directory(test_directory,\n",
        "                                                             labels='inferred',\n",
        "                                                             color_mode='grayscale',\n",
        "                                                            shuffle=True,\n",
        "                                                            batch_size=batch_size,\n",
        "                                                            image_size=image_size)\n",
        "                                                            \n",
        "validation_generator = tf.keras.utils.image_dataset_from_directory(validation_directory,\n",
        "                                                                   labels='inferred',\n",
        "                                                                   color_mode='grayscale',\n",
        "                                                                 shuffle=True,\n",
        "                                                                 batch_size=batch_size,\n",
        "                                                                 image_size=image_size)\n",
        "                                                            \n",
        "class_names = train_generator.class_names\n",
        "\n",
        "print('Number of training batches: %d' % tf.data.experimental.cardinality(train_generator).numpy())\n",
        "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_generator).numpy())\n",
        "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_generator).numpy())\n"
      ],
      "metadata": {
        "id": "Y9J987OHjSOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e53d14-2aaf-42f4-a846-a5348b74026f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 files belonging to 2 classes.\n",
            "Found 300 files belonging to 2 classes.\n",
            "Found 600 files belonging to 2 classes.\n",
            "Number of training batches: 250\n",
            "Number of test batches: 38\n",
            "Number of validation batches: 75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "datagen_train = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = False \n",
        ")\n",
        "\n",
        "datagen_test = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "datagen_validation = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = datagen_train.flow_from_directory(\n",
        "    directory = train_directory,\n",
        "    target_size = image_size,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    color_mode='grayscale',\n",
        "    follow_links=True\n",
        "    )\n",
        "\n",
        "test_generator = datagen_test.flow_from_directory(\n",
        "    directory = test_directory,\n",
        "    target_size = image_size,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    color_mode='grayscale',\n",
        "    follow_links=True\n",
        "    )\n",
        "\n",
        "validation_generator = datagen_validation.flow_from_directory(\n",
        "    directory = validation_directory,\n",
        "    target_size = image_size,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    color_mode='grayscale',\n",
        "    follow_links=True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om6rqWUL0uRD",
        "outputId": "564174ad-66e7-40d1-ab20-e544fdd1b322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 300 images belonging to 2 classes.\n",
            "Found 600 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Flatten, Dropout, Conv2D , MaxPooling2D\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(28, 28, 1), name='input')\n",
        "\n",
        "main_branch = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=1)(inputs)\n",
        "main_branch = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(main_branch)\n",
        "main_branch = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=1)(main_branch)\n",
        "main_branch = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(main_branch)\n",
        "main_branch = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), strides=1)(main_branch)\n",
        "main_branch = tf.keras.layers.Flatten()(main_branch)\n",
        "main_branch = tf.keras.layers.Dense(512, activation='relu')(main_branch)\n",
        "\n",
        "\n",
        "task_1_branch = tf.keras.layers.Dense(512, activation='relu')(main_branch)\n",
        "task_1_branch = tf.keras.layers.Dense(256, activation='relu')(main_branch)\n",
        "task_1_branch = tf.keras.layers.Dense(128, activation='relu')(task_1_branch)\n",
        "task_1_branch = tf.keras.layers.Dense(10, activation='softmax', name='task_1_output')(task_1_branch)\n",
        "\n",
        "\n",
        "task_2_branch = tf.keras.layers.Dense(512, activation='relu')(main_branch)\n",
        "task_2_branch = tf.keras.layers.Dense(256, activation='relu')(task_2_branch)\n",
        "task_2_branch = tf.keras.layers.Dense(100, activation='relu')(task_2_branch)\n",
        "task_2_branch = tf.keras.layers.Dense(2, activation='sigmoid', name='task_2_output')(task_2_branch)\n",
        "\n",
        "model = tf.keras.Model(inputs = inputs, outputs = [task_1_branch, task_2_branch])"
      ],
      "metadata": {
        "id": "na6LkV5V1HDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=1e-5)\n",
        "# loss = 'categorical_crossentropy'\n",
        "\n",
        "loss={'task_1_output': 'categorical_crossentropy',\n",
        "      'task_2_output': 'binary_crossentropy'}\n",
        "\n",
        "# metrics = ['categorical_accuracy']\n",
        "#metrics=[tf.keras.metrics.Accuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zETu8m641cso",
        "outputId": "7e616f3a-aead-4755-e411-fcb457b04412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
            "  opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2yExuK01z56",
        "outputId": "89245992-5c35-4efe-f4f8-b8d613e9c984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 26, 26, 32)   320         ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 13, 13, 32)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 11, 11, 64)   18496       ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 5, 5, 64)    0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 3, 3, 128)    73856       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1152)         0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          590336      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 512)          262656      ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 256)          131328      ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 256)          131328      ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 128)          32896       ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 100)          25700       ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " task_1_output (Dense)          (None, 10)           1290        ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " task_2_output (Dense)          (None, 2)            202         ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,268,408\n",
            "Trainable params: 1,268,408\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "q58lASqi2X57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/dexterfichuk/GoogleDriveCheckpoint/master/google_drive_checkpoint.py -O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jHeKTgi2aqq",
        "outputId": "59b5885f-a9b9-463d-d934-4b61370ddfa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4522  100  4522    0     0  30972      0 --:--:-- --:--:-- --:--:-- 30972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install httplib2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNhuXrrZ2cDW",
        "outputId": "190a6b5e-043f-418c-e606-ba16e7871a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.7/dist-packages (0.17.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-api-python-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXM1ueVt2dcc",
        "outputId": "61373d04-d6b6-496e-b0c0-93e9e305c2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (1.12.11)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (1.35.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (0.0.4)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (1.15.0)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (1.31.6)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client) (2022.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client) (1.56.4)\n",
            "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client) (3.17.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client) (21.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client) (0.2.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3dev,>=1.21.0->google-api-python-client) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google_drive_checkpoint import GoogleDriveCheckpoint\n",
        "\n",
        "checkpoint = GoogleDriveCheckpoint('testModel.h5', drive, save_best_only=True, \n",
        "                                   save_weights_only=False, \n",
        "                                   monitor = 'val_accuracy', mode = 'auto',\n",
        "                                   verbose=1)"
      ],
      "metadata": {
        "id": "X6cNrYI314f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "earlystop = EarlyStopping(\n",
        "    monitor = 'val_accuracy', \n",
        "    min_delta = 0, #val_acc\n",
        "    patience = 10, \n",
        "    verbose = 1, \n",
        "    mode = 'auto'\n",
        ")"
      ],
      "metadata": {
        "id": "F_XAVwL0194R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "# steps_per_epoch = (train_generator.n / batch_size)\n",
        "# steps_test = test_generator.n / batch_size\n",
        "# steps_validation  = validation_generator.n / batch_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit_generator(\n",
        "    generator=train_generator,\n",
        "    epochs = epochs, \n",
        "    # steps_per_epoch = steps_per_epoch,\n",
        "    validation_data = validation_generator,\n",
        "    # validation_steps = steps_test,\n",
        "    callbacks = [checkpoint, earlystop]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "ZbwX3TFw1_53",
        "outputId": "b9f8fed1-02a0-44fd-8f81-41b92d2bc0c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-86acf19ae96a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# validation_steps = steps_test,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearlystop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2221\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2223\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1790, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title(\"Model Accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Train Accuracy\",\"Validation Accuracy\"], loc = 'lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yyoAVW2X2CDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"Model Loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Train Loss\",\"Validation Loss\"], loc = 'upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qPhcg7JV2DW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finaltestCNV_dir = test_directory\n",
        "datagen_finaltestCNV_dir = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "generator_finaltestCNV_dir = datagen_finaltestCNV_dir.flow_from_directory(directory=finaltestCNV_dir,\n",
        "                                                  target_size=image_size,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=False)\n",
        "steps_finaltestCNV_dir = generator_finaltestCNV_dir.n / batch_size\n",
        "\n",
        "y_pred = model.predict_generator(generator_finaltestCNV_dir,steps = steps_finaltestCNV_dir)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "x8blsXID2GXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class_indices=np.argmax(y_pred,axis=1)\n",
        "print(predicted_class_indices)"
      ],
      "metadata": {
        "id": "aWJ2gmOb2ILI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_label = []\n",
        "ADILabel = 0\n",
        "BACKLabel = 1\n",
        "DEBLabel = 2\n",
        "LYMLabel = 3\n",
        "MUCLabel = 4\n",
        "MUSLabel = 5\n",
        "NORMLabel = 6\n",
        "STRLabel = 7\n",
        "TUMLabel = 8\n",
        "\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    test_label.append(ADILabel)\n",
        "\n",
        "for i in range(1069):\n",
        "    test_label.append(BACKLabel)\n",
        "\n",
        "for i in range(1207):\n",
        "    test_label.append(DEBLabel)\n",
        "\n",
        "for i in range(1233):\n",
        "    test_label.append(LYMLabel)\n",
        "\n",
        "for i in range(1026):\n",
        "    test_label.append(MUCLabel)\n",
        "\n",
        "\n",
        "for i in range(1364):\n",
        "    test_label.append(MUSLabel)\n",
        "\n",
        "\n",
        "for i in range(877):\n",
        "    test_label.append(NORMLabel)\n",
        "\n",
        "for i in range(1164):\n",
        "    test_label.append(STRLabel)\n",
        "\n",
        "\n",
        "for i in range(1452):\n",
        "    test_label.append(TUMLabel)\n",
        "\n",
        "\n",
        "print(test_label)"
      ],
      "metadata": {
        "id": "SEveyLrb2J5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import itertools"
      ],
      "metadata": {
        "id": "4wqGtHKa2La_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "df_cm = pd.DataFrame(array, index = [\"ADI\", \"BACK\",\"DEB\",\"LYM\",\"MUC\",\"MUS\",\"NORM\",\"STR\",\"TUM\"],\n",
        "                  columns = [\"ADI\", \"BACK\",\"DEB\",\"LYM\",\"MUC\",\"MUS\",\"NORM\",\"STR\",\"TUM\"])\n",
        "plt.figure(figsize = (10,10))\n",
        "sn.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')"
      ],
      "metadata": {
        "id": "_AjaX54-2Msn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}